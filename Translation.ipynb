{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> This notebook contains the Ipython version of the translation.py file which is useful when executing code cells in Google Collaboratory for Model training. \n",
    " \n",
    "<h4> There is some additional boilerplate code here that makes the file suitable for running on Google Collab. If training on a local machine, these code cells can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure Tensorflow Version > 1.10. Mine is 1.12.0\n"
     ]
    }
   ],
   "source": [
    "# %load translation.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Fri Nov 30 10:39:37 2018\n",
    "\n",
    "@author: stenatu\n",
    "\"\"\"\n",
    "from __future__ import division, absolute_import, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"Make sure Tensorflow Version > 1.10. Mine is {}\".format(tf.__version__))\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# If file is not downloaded, download the file using Keras as below\n",
    "#path_to_zip = tf.keras.utils.get_file('ron-eng.zip', origin = 'http://www.manythings.org/anki/ron-eng.zip',\n",
    "#                                     extract = True)\n",
    "path_to_zip = os.getcwd()\n",
    "path_to_file = path_to_zip + '/ron-eng/ron.txt'\n",
    "\n",
    "# Clean the dataset by removing special characters\n",
    "\n",
    "def unicode_to_ascii(a):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', a)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!,?])\", r\" \\l \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    \n",
    "    s = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", s)\n",
    "    s = s.rstrip().strip()\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>' # add start and end tokens - note the spacing between start and end tokens \n",
    "    return s\n",
    "\n",
    "# preprocess the entire text and get the dataset for training. Use num examples to reduce the number of\n",
    "# training steps. Naturally more the training examples, better the model.    \n",
    "\n",
    "def create_dataset(path, num_examples = 10000):\n",
    "    text = open(file = path_to_file, encoding = 'UTF-8').read().strip().split('\\n')\n",
    "    if num_examples < 10000:\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in text]\n",
    "    else:\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in text[:10000]]\n",
    "\n",
    "    return word_pairs\n",
    "\n",
    "pairs = create_dataset(path_to_zip)\n",
    "\n",
    "# create a Language class that you can use in different contexts\n",
    "class LanguageIndex():\n",
    "    '''Creates a mapping between words and numbers. word --> index (e.g. \"cat\" --> 3) and vice-versa.\n",
    "    Use this class more generally for text generation models.'''\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        \n",
    "        self.create_index()\n",
    "    \n",
    "    def create_index(self):\n",
    "        ''' create a vocabulary and an word to index dictionary and vice-versa'''\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "        \n",
    "        self.vocab = sorted(self.vocab)\n",
    "        \n",
    "        self.word2idx['<pad>'] = 0\n",
    "        self.idx2word[0] = '<pad>' \n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1 #because 0 is reserved for pad.\n",
    "            self.idx2word[index] = word\n",
    "    # this returns a LanguageIndex object. Aternatively put return statements \n",
    "    # for the function and just call the create_index functionn. \n",
    "        \n",
    "\n",
    "# Load the dataset for english to Romanian translation\n",
    "def max_length(tensor):\n",
    "    return max([len(t) for t in tensor])\n",
    "    \n",
    "def load_dataset(pairs):\n",
    "    inp_lang = LanguageIndex(en for en, rom in pairs)\n",
    "    tar_lang = LanguageIndex(rom for en, rom in pairs)\n",
    "      \n",
    "      # get the index tensor.\n",
    "    input_tensor = [[inp_lang.word2idx[word] for word in en.split(' ')] for en, rom in pairs]\n",
    "    target_tensor = [[tar_lang.word2idx[word] for word in rom.split(' ')] for en, rom in pairs]\n",
    "    \n",
    "      # each of these sublists are of a different length. So we pad them by max_length\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen = max_length_inp\n",
    "                                                                  , padding = 'post')\n",
    "      \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, maxlen = max_length_tar\n",
    "                                                                  , padding = 'post')\n",
    "    \n",
    "    \n",
    "    return inp_lang, tar_lang, input_tensor, target_tensor, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<end>', '<start>', 'a', 'aback', 'abandoned', 'abated', 'abating', 'aberration', 'abhor']\n",
      "Max Length English = 42, Max Length Romanian 48\n",
      "Shape of input tensor = (8215, 42)\n",
      "Shape of target tensor = (8215, 48)\n",
      "The dataset shape is ----> <BatchDataset shapes: ((64, 42), (64, 48)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "inp_lang, tar_lang, input_tensor, target_tensor, max_length_inp, max_length_tar = load_dataset(pairs)\n",
    "print(list(inp_lang.word2idx)[:10])\n",
    "#print(list(tar_lang.word2idx)[:5])\n",
    "    \n",
    "# Check that the tensors have the corect shape\n",
    "print(\"Max Length English = {}, Max Length Romanian {}\".format(max_length_inp, max_length_tar))\n",
    "print(\"Shape of input tensor = {}\".format(np.shape(input_tensor)))   \n",
    "print(\"Shape of target tensor = {}\".format(np.shape(target_tensor))) \n",
    "\n",
    "# Create the test, train split to train the model. USe a random 80-20 split here.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n",
    "                                                                                               target_tensor,\n",
    "                                                                                               test_size = 0.2\n",
    "                                                                                                )\n",
    "\n",
    "## Create a tf.data dataset which takes data in batches for training. \n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 10\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(tar_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Check that the dataset shape object has the right dimensions -- inputs of Batch_size, max_english and output \n",
    "# Batch_size, max_romanian\n",
    "\n",
    "print(\"The dataset shape is ----> {}\".format(dataset))\n",
    "\n",
    "# Model \n",
    "\n",
    "def gru(units):\n",
    "    ''' If you have a GPU , this model defaults to a CuDNNGRU or else a GRU '''\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_initializer ='glorot_uniform')\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_activation='sigmoid',\n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    ''' Writes the Encoder Class used for training '''\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder,self).__init__() #super class as they both subclass tf.keras.Model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.enc_units = enc_units\n",
    "        self.batch_size = batch_size\n",
    "        self.gru = gru(self.enc_units)\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # input vector has shape (batch_size, max_input_length)\n",
    "        x = self.embedding(x)\n",
    "        # embedded vector has shape (batch_size, max_input_length, embedding_dim)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        # output_shape = [batch_size, max_input_length, enc_units]\n",
    "        # state = [batch_size, enc_units]\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))\n",
    "    \n",
    "  # default attention is Bahdanau  \n",
    "class Decoder(tf.keras.Model, attention = 'Bahdanau'):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.dec_units = dec_units\n",
    "        self.batch_size = batch_size\n",
    "        self.gru = gru(units = self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # define the attention weights for Bahndau attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V =tf.keras.layers.Dense(1)\n",
    "    \n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \n",
    "        #enc_output shape = [batch_size, max_input_length, enc_units].\n",
    "        \n",
    "        # In below we want to compute the attention weights for each word which comes in the \n",
    "        # max_input_length dimension. FInally we want to sum over that dimension to get the context\n",
    "        # vector.\n",
    "        \n",
    "        # first implement attention mechanism\n",
    "        \n",
    "        # compute score from hidden state of decoder and the current output of the encoder\n",
    "        # at that timestep.\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # this is essential so that hidden has the same shape as output of the encoder.\n",
    "        \n",
    "        #score_shape = (batch_size, max_length, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        # the output of the score is [batch_size, max_length, 1]\n",
    "        \n",
    "        # from the score construct the attention weights\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        #sum over all the attention weights over all the hidden states \n",
    "        # after unrolling through time.\n",
    "        #context_vector has shape (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        \n",
    "        # code to get the hidden states of the target vectors\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x has shape -- [batch_size, 1, embedding_dim] -- 1 because each word is being compared\n",
    "        # to the input to compute the attention vector\n",
    "        \n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        # x after concat has shape [batch_size, 1, hidden_size + embedding_dim]\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # output shape  = [batch_size*1, batch_size+embedding_dim]\n",
    "        \n",
    "        x = self.fc(output)\n",
    "        # shape of x = [batch_size, vocab_size]\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        tf.zeros([self.batch_size, self.dec_units])\n",
    "    \n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.4641\n",
      "Epoch 1 Batch 100 Loss 1.2240\n",
      "Epoch 1 Loss 1.4461\n",
      "Time taken for 1 epoch 105.57359623908997 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the Optimizer and the Loss\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1-np.equal(real, 0)\n",
    "    loss_red = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = real, logits=pred)*mask\n",
    "    return tf.reduce_mean(loss_red)\n",
    "\n",
    "# Checkpoints for Model Serving\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss= 0\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden= encoder(inp, hidden)\n",
    "           \n",
    "            dec_hidden = enc_hidden\n",
    "           \n",
    "           # initial input is pad - multiply by Batchsize to create that vector, expand dims to match the needed input dimensions\n",
    "            dec_input = tf.expand_dims([tar_lang.word2idx['<pad>']]*BATCH_SIZE, 1)\n",
    "           #print(targ.shape)\n",
    "            for t in range(1, targ.shape[1]):\n",
    "               #generate predictions from the decoder model -- which uses decoder_input, hidden and enc_output\n",
    "               predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "               \n",
    "               loss += loss_function(targ[:, t], predictions)\n",
    "               \n",
    "               # teacher forcing -- put the target word as input to decoder\n",
    "               \n",
    "               dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "              \n",
    "        batch_loss = (loss/int(targ.shape[1]))\n",
    "       \n",
    "        total_loss += batch_loss\n",
    "       \n",
    "        variables = encoder.variables + decoder.variables\n",
    "       \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "       \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "       \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy()))\n",
    "           \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence,encoder, decoder, inp_lang, tar_lang, max_length_inp, max_length_tar):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen = max_length_inp, padding = 'post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    # Make a batch of 1\n",
    "    dec_input = tf.expand_dims([tar_lang.word2idx['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_tar):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += tar_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if tar_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        \n",
    "    dec_input = tf.expand_dims([predicted_id], 0) # put the predicted id back into the model\n",
    "    \n",
    "    return result,sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> this is an aberration <end>\n",
      "Predicted translation: <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> <start> \n"
     ]
    }
   ],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, tar_lang, max_length_inp, max_length_tar):\n",
    "    result, sentence = evaluate(sentence, encoder, decoder, inp_lang, tar_lang, max_length_inp, max_length_tar)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "# Restore the model from the latest checkpoint and evaluate on some sentences\n",
    "    \n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "translate('this is an aberration', encoder, decoder, \n",
    "        inp_lang, tar_lang, max_length_inp, max_length_tar)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
