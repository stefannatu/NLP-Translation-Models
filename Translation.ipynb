{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwB-FBYb_Aqu"
   },
   "source": [
    "<h3> This notebook contains the Ipython version of the translation.py file which is useful when executing code cells in Google Collaboratory for Model training. \n",
    " \n",
    "<h4> There is some additional boilerplate code here that makes the file suitable for running on Google Collab. If training on a local machine, these code cells can be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "kgYAtA7X_Aqv",
    "outputId": "3b7cf360-f5b0-4c7e-b2c0-d45886b513b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_k8FljTc_WeN",
    "outputId": "1d9456cf-9960-4e8c-fb4e-9432170672c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ron-eng  training_checkpoints  Translation.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/My Drive/Colab Notebooks/Neural_Machine_Translation')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bCbznh4m_Aqz",
    "outputId": "356341ab-a959-4f3c-b9b9-f67d2dafe682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure Tensorflow Version > 1.10. Mine is 1.12.0\n"
     ]
    }
   ],
   "source": [
    "# %load translation.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Created on Fri Nov 30 10:39:37 2018\n",
    "\n",
    "@author: stenatu\n",
    "\"\"\"\n",
    "from __future__ import division, absolute_import, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"Make sure Tensorflow Version > 1.10. Mine is {}\".format(tf.__version__))\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# If file is not downloaded, download the file using Keras as below\n",
    "#path_to_zip = tf.keras.utils.get_file('ron-eng.zip', origin = 'http://www.manythings.org/anki/ron-eng.zip',\n",
    "#                                     extract = True)\n",
    "path_to_zip = os.getcwd()\n",
    "path_to_file = path_to_zip + '/ron-eng/ron.txt'\n",
    "\n",
    "# Clean the dataset by removing special characters\n",
    "\n",
    "def unicode_to_ascii(a):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', a)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!,?])\", r\" \\l \", s)\n",
    "    s = re.sub(r'[\" \"]+', \" \", s)\n",
    "    \n",
    "    s = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", s)\n",
    "    s = s.rstrip().strip()\n",
    "    \n",
    "    s = '<start> ' + s + ' <end>' # add start and end tokens - note the spacing between start and end tokens \n",
    "    return s\n",
    "\n",
    "# preprocess the entire text and get the dataset for training. Use num examples to reduce the number of\n",
    "# training steps. Naturally more the training examples, better the model.    \n",
    "\n",
    "def create_dataset(path, num_examples = 10000):\n",
    "    text = open(file = path_to_file, encoding = 'UTF-8').read().strip().split('\\n')\n",
    "    if num_examples < 10000:\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in text]\n",
    "    else:\n",
    "        word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in text[:10000]]\n",
    "\n",
    "    return word_pairs\n",
    "\n",
    "pairs = create_dataset(path_to_zip)\n",
    "\n",
    "# create a Language class that you can use in different contexts\n",
    "class LanguageIndex():\n",
    "    '''Creates a mapping between words and numbers. word --> index (e.g. \"cat\" --> 3) and vice-versa.\n",
    "    Use this class more generally for text generation models.'''\n",
    "    def __init__(self, lang):\n",
    "        self.lang = lang\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.vocab = set()\n",
    "        \n",
    "        self.create_index()\n",
    "    \n",
    "    def create_index(self):\n",
    "        ''' create a vocabulary and an word to index dictionary and vice-versa'''\n",
    "        for phrase in self.lang:\n",
    "            self.vocab.update(phrase.split(' '))\n",
    "        \n",
    "        self.vocab = sorted(self.vocab)\n",
    "        \n",
    "        self.word2idx['<pad>'] = 0 \n",
    "        for index, word in enumerate(self.vocab):\n",
    "            self.word2idx[word] = index + 1 #because 0 is reserved for pad.\n",
    "            \n",
    "        for word, index in self.word2idx.items():\n",
    "          self.idx2word[index] = word\n",
    "    # this returns a LanguageIndex object. Aternatively put return statements \n",
    "    # for the function and just call the create_index functionn. \n",
    "        \n",
    "\n",
    "# Load the dataset for english to Romanian translation\n",
    "def max_length(tensor):\n",
    "    return max([len(t) for t in tensor])\n",
    "    \n",
    "def load_dataset(pairs):\n",
    "    inp_lang = LanguageIndex(en for en, rom in pairs)\n",
    "    tar_lang = LanguageIndex(rom for en, rom in pairs)\n",
    "      \n",
    "      # get the index tensor.\n",
    "    input_tensor = [[inp_lang.word2idx[word] for word in en.split(' ')] for en, rom in pairs]\n",
    "    target_tensor = [[tar_lang.word2idx[word] for word in rom.split(' ')] for en, rom in pairs]\n",
    "    \n",
    "      # each of these sublists are of a different length. So we pad them by max_length\n",
    "    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n",
    "    \n",
    "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, maxlen = max_length_inp\n",
    "                                                                  , padding = 'post')\n",
    "      \n",
    "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, maxlen = max_length_tar\n",
    "                                                                  , padding = 'post')\n",
    "    \n",
    "    \n",
    "    return inp_lang, tar_lang, input_tensor, target_tensor, max_length_inp, max_length_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "Gy-NgEO6QDQj",
    "outputId": "6916dc0f-10d0-470d-e5f4-edfe7479b929"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<start> i m numb l <end>', '<start> sunt indiferenta l <end>'],\n",
       " ['<start> i m sick l <end>', '<start> sunt bolnav l <end>'],\n",
       " ['<start> it hurts l <end>', '<start> ma doare l <end>'],\n",
       " ['<start> it s tom l <end>', '<start> este tom l <end>'],\n",
       " ['<start> marry me l <end>', '<start> casatoreste te cu mine l <end>'],\n",
       " ['<start> may i go l <end>', '<start> pot sa merg l <end>'],\n",
       " ['<start> may i go l <end>', '<start> pot sa ma duc l <end>'],\n",
       " ['<start> terrific l <end>', '<start> teribil l <end>'],\n",
       " ['<start> too late l <end>', '<start> prea tarziu l <end>'],\n",
       " ['<start> trust me l <end>', '<start> ai incredere in mine l <end>']]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jPNV2R5yHRcC"
   },
   "outputs": [],
   "source": [
    "inp_lang, tar_lang, input_tensor, target_tensor, max_length_inp, max_length_tar = load_dataset(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "SovDYwia_Aq3",
    "outputId": "91fb065c-b5ea-4562-8b6e-b26dc496d80b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Length English = 42, Max Length Romanian 48\n",
      "Shape of input tensor = (8215, 42)\n",
      "Shape of target tensor = (8215, 48)\n",
      "The dataset shape is ----> <BatchDataset shapes: ((64, 42), (64, 48)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# Check that the tensors have the corect shape\n",
    "print(\"Max Length English = {}, Max Length Romanian {}\".format(max_length_inp, max_length_tar))\n",
    "print(\"Shape of input tensor = {}\".format(np.shape(input_tensor)))   \n",
    "print(\"Shape of target tensor = {}\".format(np.shape(target_tensor))) \n",
    "\n",
    "# Create the test, train split to train the model. USe a random 80-20 split here.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor,\n",
    "                                                                                               target_tensor,\n",
    "                                                                                               test_size = 0.2\n",
    "                                                                                                )\n",
    "\n",
    "## Create a tf.data dataset which takes data in batches for training. \n",
    "\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "\n",
    "embedding_dim = 256\n",
    "units = 300\n",
    "vocab_inp_size = len(inp_lang.word2idx)\n",
    "vocab_tar_size = len(tar_lang.word2idx)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# Check that the dataset shape object has the right dimensions -- inputs of Batch_size, max_english and output \n",
    "# Batch_size, max_romanian\n",
    "\n",
    "print(\"The dataset shape is ----> {}\".format(dataset))\n",
    "\n",
    "# Model \n",
    "\n",
    "def gru(units):\n",
    "    ''' If you have a GPU , this model defaults to a CuDNNGRU or else a GRU '''\n",
    "    if tf.test.is_gpu_available():\n",
    "        return tf.keras.layers.CuDNNGRU(units, \n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_initializer ='glorot_uniform')\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, \n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        recurrent_activation='sigmoid',\n",
    "                                        recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    ''' Writes the Encoder Class used for training '''\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder,self).__init__() #super class as they both subclass tf.keras.Model\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.enc_units = enc_units\n",
    "        self.batch_size = batch_size\n",
    "        self.gru = gru(self.enc_units)\n",
    "    \n",
    "    def call(self, x, hidden):\n",
    "        # input vector has shape (batch_size, max_input_length)\n",
    "        x = self.embedding(x)\n",
    "        # embedded vector has shape (batch_size, max_input_length, embedding_dim)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        # output_shape = [batch_size, max_input_length, enc_units]\n",
    "        # state = [batch_size, enc_units]\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size, self.enc_units))\n",
    "    \n",
    "  # default attention is Bahdanau  \n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.dec_units = dec_units\n",
    "        self.batch_size = batch_size\n",
    "        self.gru = gru(units = self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # define the attention weights for Bahndau attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V =tf.keras.layers.Dense(1)\n",
    "    \n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        \n",
    "        #enc_output shape = [batch_size, max_input_length, enc_units].\n",
    "        \n",
    "        # In below we want to compute the attention weights for each word which comes in the \n",
    "        # max_input_length dimension. FInally we want to sum over that dimension to get the context\n",
    "        # vector.\n",
    "        \n",
    "        # first implement attention mechanism\n",
    "        \n",
    "        # compute score from hidden state of decoder and the current output of the encoder\n",
    "        # at that timestep.\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        # this is essential so that hidden has the same shape as output of the encoder.\n",
    "        \n",
    "        #score_shape = (batch_size, max_length, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        # the output of the score is [batch_size, max_length, 1]\n",
    "        \n",
    "        # from the score construct the attention weights\n",
    "        attention_weights = tf.nn.softmax(score, axis = 1)\n",
    "        \n",
    "        #sum over all the attention weights over all the hidden states \n",
    "        # after unrolling through time.\n",
    "        #context_vector has shape (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
    "        \n",
    "        \n",
    "        # code to get the hidden states of the target vectors\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x has shape -- [batch_size, 1, embedding_dim] -- 1 because each word is being compared\n",
    "        # to the input to compute the attention vector\n",
    "        \n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
    "        # x after concat has shape [batch_size, 1, hidden_size + embedding_dim]\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # output shape  = [batch_size*1, batch_size+embedding_dim]\n",
    "        \n",
    "        x = self.fc(output)\n",
    "        # shape of x = [batch_size, vocab_size]\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        tf.zeros([self.batch_size, self.dec_units])\n",
    "    \n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "auqdOzMv6g8L"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1-np.equal(real, 0)\n",
    "    loss_red_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_red_)\n",
    "\n",
    "# Checkpoints for Model Serving\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "id": "53q57OKm_Aq8",
    "outputId": "ef8f30cf-5eba-4a13-8ddf-d57761e6bdc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.4585\n",
      "Epoch 1 Batch 100 Loss 1.0019\n",
      "Epoch 1 Loss 1.0551\n",
      "Time taken for 1 epoch 105.93933916091919 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8880\n",
      "Epoch 2 Batch 100 Loss 0.8666\n",
      "Epoch 2 Loss 0.9071\n",
      "Time taken for 1 epoch 102.88025569915771 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.9305\n",
      "Epoch 3 Batch 100 Loss 0.7751\n",
      "Epoch 3 Loss 0.8627\n",
      "Time taken for 1 epoch 102.91278052330017 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.9280\n",
      "Epoch 4 Batch 100 Loss 0.9088\n",
      "Epoch 4 Loss 0.8258\n",
      "Time taken for 1 epoch 102.2088828086853 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.9027\n",
      "Epoch 5 Batch 100 Loss 0.7813\n",
      "Epoch 5 Loss 0.7888\n",
      "Time taken for 1 epoch 102.09615921974182 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.7620\n",
      "Epoch 6 Batch 100 Loss 0.7694\n",
      "Epoch 6 Loss 0.7511\n",
      "Time taken for 1 epoch 102.34382653236389 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.7371\n",
      "Epoch 7 Batch 100 Loss 0.6729\n",
      "Epoch 7 Loss 0.7154\n",
      "Time taken for 1 epoch 102.34416365623474 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.7204\n",
      "Epoch 8 Batch 100 Loss 0.6465\n",
      "Epoch 8 Loss 0.6774\n",
      "Time taken for 1 epoch 102.15769624710083 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.6250\n",
      "Epoch 9 Batch 100 Loss 0.6585\n",
      "Epoch 9 Loss 0.6379\n",
      "Time taken for 1 epoch 102.76401162147522 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.5792\n",
      "Epoch 10 Batch 100 Loss 0.5421\n",
      "Epoch 10 Loss 0.5993\n",
      "Time taken for 1 epoch 102.58771848678589 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the Optimizer and the Loss\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss= 0\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden= encoder(inp, hidden)\n",
    "           \n",
    "            dec_hidden = enc_hidden\n",
    "           \n",
    "           # initial input is pad - multiply by Batchsize to create that vector, expand dims to match the needed input dimensions\n",
    "            dec_input = tf.expand_dims([tar_lang.word2idx['<start>']]*BATCH_SIZE, 1)\n",
    "           #print(targ.shape)\n",
    "            for t in range(1, targ.shape[1]):\n",
    "               #generate predictions from the decoder model -- which uses decoder_input, hidden and enc_output\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "               \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "               \n",
    "               # teacher forcing -- put the target word as input to decoder\n",
    "               \n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "              \n",
    "        batch_loss = (loss/int(targ.shape[1]))\n",
    "       \n",
    "        total_loss += batch_loss\n",
    "       \n",
    "        variables = encoder.variables + decoder.variables\n",
    "       \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "       \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "       \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, \n",
    "                                                         batch, batch_loss.numpy()))\n",
    "           \n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-WY_1dPt_Aq-"
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence,encoder, decoder, inp_lang, tar_lang, max_length_inp, max_length_tar):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    \n",
    "    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n",
    "    \n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen = max_length_inp, padding = 'post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    \n",
    "    \n",
    "    dec_hidden = enc_hidden\n",
    "    # Make a batch of 1\n",
    "    dec_input = tf.expand_dims([tar_lang.word2idx['<start>']], 0)\n",
    "    \n",
    "    for t in range(max_length_tar):\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        #print(predicted_id)\n",
    "        result += tar_lang.idx2word[predicted_id] + ' '\n",
    "\n",
    "        if tar_lang.idx2word[predicted_id] == '<end>':\n",
    "            return result, sentence\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0) # put the predicted id back into the model\n",
    "    \n",
    "    return result,sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "2gUXVDVs_ArB",
    "outputId": "472da9e5-dbdd-4132-bffc-85ede3543b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> what is your name l <end>\n",
      "Predicted translation: ei este foarte bine l <end> \n"
     ]
    }
   ],
   "source": [
    "def translate(sentence, encoder, decoder, inp_lang, tar_lang, max_length_inp, max_length_tar):\n",
    "    result, sentence = evaluate(sentence, encoder, decoder, inp_lang, tar_lang, max_length_inp, max_length_tar)\n",
    "        \n",
    "    print('Input: {}'.format(sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "# Restore the model from the latest checkpoint and evaluate on some sentences\n",
    "    \n",
    "translate('what is your name.', encoder, decoder, \n",
    "        inp_lang, tar_lang, max_length_inp, max_length_tar)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k2PqmGCU_ArE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Translation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
